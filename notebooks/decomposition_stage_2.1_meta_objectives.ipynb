{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801fbf4e",
   "metadata": {},
   "source": [
    "## Task 2.1. LLM-Assisted Meta-Objective Definition\n",
    "\n",
    "**Purpose:** To assist the Operative in distilling a project's core purpose into a set of compelling Meta-Objective statements, formatted for easy inclusion in input_ops.json.\n",
    "\n",
    "**User Inputs to LLM:**\n",
    "* Project Name/Concept\n",
    "* Raw Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95591e5a",
   "metadata": {},
   "source": [
    "You are an expert project analyst working with Operative Kin-Caid of the Chiron Guild.\n",
    "\n",
    "**Your Current Task:** Meta-Objective Definition.\n",
    "Your goal is to assist Operative Kin-Caid in drafting 3-5 concise, compelling Meta-Objective statements based on the provided project information.\n",
    "\n",
    "**Process:**\n",
    "1.  You will be provided with a Project Name/Concept and a Raw Project Description.\n",
    "2.  Based on this input, draft 3-5 Meta-Objective statements.\n",
    "3.  Each statement should be a single sentence capturing the absolute core purpose and primary value proposition of the project for the Chiron Guild or the specific Operative endeavor.\n",
    "4.  Focus on the ultimate 'win' or desired future state.\n",
    "5.  **Output Format:** Structure your response as a **JSON array**. Each object in the array should represent one Meta-Objective and MUST contain the following keys:\n",
    "    *   `\"id\"` (string): A short identifier (e.g., \"M1\", \"M2\", \"M3\").\n",
    "    *   `\"text\"` (string): The concise Meta-Objective statement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a13f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure Gemini (reads from Codespace secret)\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Define model configuration (match generate_briefs_for_review.py)\n",
    "GEMINI_MODEL_NAME = \"gemini-2.5-flash-preview-05-20\" \n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 1,\n",
    "    \"top_k\": 1,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "safety_settings = [\n",
    "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
    "]\n",
    "\n",
    "# --- INPUTS FOR THIS TASK ---\n",
    "project_name_concept = \"Chiron Guild Internal Communications Hub\"\n",
    "raw_project_description = \"\"\"\n",
    "The Chiron Guild currently uses a mix of Discord, email, and shared documents for internal communication. This is becoming fragmented and inefficient as we grow. We need a centralized, self-hosted platform that integrates chat, knowledge base, and announcements. The platform should be secure, extensible, and align with our values of data sovereignty and transparency. The goal is to improve information flow, reduce time spent searching for information, and foster a stronger sense of community among Chironians. This will ultimately make our cooperative more effective and resilient.\n",
    "\"\"\"\n",
    "\n",
    "# --- CONSTRUCT THE PROMPT ---\n",
    "# You'll paste the LLM Prompt from project_decomposition.md (Task 2.1) here.\n",
    "# Replace the dynamic parts with your actual inputs.\n",
    "llm_prompt_template_str = \"\"\"\n",
    "You are an expert project analyst working with Operative Kin-Caid of the Chiron Guild.\n",
    "\n",
    "**Your Current Task:** Meta-Objective Definition.\n",
    "Your goal is to assist Operative Kin-Caid in drafting 3-5 concise, compelling Meta-Objective statements based on the provided project information.\n",
    "\n",
    "**Process:**\n",
    "1.  You will be provided with a Project Name/Concept and a Raw Project Description.\n",
    "2.  Based on this input, draft 3-5 Meta-Objective statements.\n",
    "3.  Each statement should be a single sentence capturing the absolute core purpose and primary value proposition of the project for the Chiron Guild or the specific Operative endeavor.\n",
    "4.  Focus on the ultimate 'win' or desired future state.\n",
    "5.  **Output Format:** Structure your response as a **JSON array**. Each object in the array should represent one Meta-Objective and MUST contain the following keys:\n",
    "    *   `\"id\"` (string): A short identifier (e.g., \"M1\", \"M2\", \"M3\").\n",
    "    *   `\"text\"` (string): The concise Meta-Objective statement.\n",
    "\n",
    "**Chiron Guild Context Reminder:**\n",
    "The Chiron Guild is a worker-owned, AI-augmented digital cooperative with a 'Mythic Core, Precision Shell' ethos. Your analysis should align with this mission.\n",
    "\"\"\"\n",
    "\n",
    "# Combine LLM system instruction with user input\n",
    "# The actual prompt to Gemini will be simpler, just the request.\n",
    "# The \"User Input\" section from project_decomposition.md describes what *you* provide.\n",
    "# The \"LLM Prompt\" section from project_decomposition.md is the *system instruction* for Gemini.\n",
    "\n",
    "# For interactive use, the best way to handle this is a multi-turn approach or\n",
    "# concatenate the system instruction with the user's specific request.\n",
    "\n",
    "# Let's create a single prompt that Gemini can respond to based on task 2.1 description\n",
    "full_prompt_for_gemini = f\"\"\"\n",
    "{llm_prompt_template_str}\n",
    "\n",
    "---\n",
    "Project Name/Concept: {project_name_concept}\n",
    "Raw Project Description: {raw_project_description}\n",
    "\n",
    "Task: Generate the JSON array of Meta-Objectives.\n",
    "\"\"\"\n",
    "\n",
    "# --- CALL GEMINI API ---\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=GEMINI_MODEL_NAME,\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings\n",
    ")\n",
    "\n",
    "print(f\"Calling Gemini model: {GEMINI_MODEL_NAME} for Meta-Objective Definition...\")\n",
    "try:\n",
    "    response = model.generate_content(full_prompt_for_gemini)\n",
    "    \n",
    "    if response.text:\n",
    "        meta_objectives_json = json.loads(response.text)\n",
    "        print(\"\\n--- Generated Meta-Objectives (JSON) ---\")\n",
    "        print(json.dumps(meta_objectives_json, indent=2))\n",
    "        # You can now save this to a file or a variable for next steps\n",
    "        with open(\"temp_meta_objectives.json\", \"w\") as f:\n",
    "            json.dump(meta_objectives_json, f, indent=2)\n",
    "        print(\"\\nSaved to temp_meta_objectives.json for review.\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: Gemini API response was empty or did not contain text content.\")\n",
    "        print(f\"Full Gemini Response object: {response}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Gemini API call: {e}\")\n",
    "    # Print full response for debugging if it's not text\n",
    "    if hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n",
    "        print(f\"Gemini API blocked response due to: {response.prompt_feedback.block_reason}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
