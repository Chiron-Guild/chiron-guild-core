{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b1cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è  Initializing Guild Interface AI...\n",
      "‚úÖ Protocol file found and loaded from: C:\\Local Working Files\\GitHub\\chiron-guild-core\\notebooks\\..\\_Admin & Core Docs/Protocols-Frameworks/Copilot_Context_Protocol.md\n",
      "‚úÖ  Initialization Complete. AI is calibrated to Guild Protocols.\n",
      "‚û°Ô∏è  Proceed to run Cell 2 to begin the chat session.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Initialization (Local Environment Version)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables from the .env file in the root directory\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"üî¥ ERROR: GEMINI_API_KEY not found in .env file.\")\n",
    "    print(\"Please create a .env file in the repository root with GEMINI_API_KEY='your-key'.\")\n",
    "else:\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "# --- Guild Protocol Functions ---\n",
    "\n",
    "def load_system_instruction(protocol_file_path=\"_Admin & Core Docs/Protocols-Frameworks/Copilot_Context_Protocol.md\"):\n",
    "    \"\"\"Loads the Guild's core protocol document to be used as the system instruction.\"\"\"\n",
    "    try:\n",
    "        # Use os.path.join for cross-platform compatibility\n",
    "        full_path = os.path.join(os.getcwd(), '..', protocol_file_path) # Assumes notebook is in notebooks/\n",
    "        if not os.path.exists(full_path):\n",
    "             full_path = os.path.join(os.getcwd(), protocol_file_path) # Assumes notebook is in root\n",
    "        \n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            print(f\"‚úÖ Protocol file found and loaded from: {full_path}\")\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üî¥ ERROR: Protocol file not found.\")\n",
    "        print(\"Please ensure the path in the 'load_system_instruction' function is correct relative to the repository root.\")\n",
    "        return None\n",
    "\n",
    "def save_chat_history(chat_session, file_path):\n",
    "    \"\"\"Saves the chat history to a JSON file.\"\"\"\n",
    "    history = [\n",
    "        {'role': msg.role, 'parts': [part.text for part in msg.parts]}\n",
    "        for msg in chat_session.history\n",
    "    ]\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        print(f\"‚úÖ Chat history saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üî¥ ERROR saving history: {e}\")\n",
    "\n",
    "def load_chat_history(file_path):\n",
    "    \"\"\"Loads a chat history from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üü° INFO: No previous history found at {file_path}. Starting a new session.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"üî¥ ERROR loading history: {e}\")\n",
    "        return None\n",
    "\n",
    "# The full, corrected function in Cell 1\n",
    "\n",
    "def initialize_guild_chat(system_instruction, history=None):\n",
    "    \"\"\"Initializes a new chat session with system instruction and optional history.\"\"\"\n",
    "    \n",
    "    # Define the generation_config for more detailed responses\n",
    "    generation_config = {\n",
    "      \"temperature\": 0.7,\n",
    "      \"top_p\": 1,\n",
    "      \"top_k\": 32,\n",
    "      \"max_output_tokens\": 8192,\n",
    "    }\n",
    "\n",
    "    # This is the corrected line\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.5-flash-preview-05-20\",\n",
    "        system_instruction=system_instruction,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    # Pass the loaded history to the start_chat method\n",
    "    chat = model.start_chat(history=history if history else None)\n",
    "    return chat\n",
    "    \n",
    "    # Pass the loaded history to the start_chat method\n",
    "    chat = model.start_chat(history=history if history else None)\n",
    "    return chat\n",
    "    \n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32,\n",
    "        \"max_output_tokens\": 8192, # Increase the max token output significantly\n",
    "}\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.5-flash-preview-05-20\", # Corrected Model Name\n",
    "        system_instruction=system_instruction,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    chat = model.start_chat()\n",
    "    return chat\n",
    "\n",
    "# --- Main Initialization Step ---\n",
    "print(\"üõ°Ô∏è  Initializing Guild Interface AI...\")\n",
    "guild_system_protocol = load_system_instruction()\n",
    "\n",
    "if guild_system_protocol:\n",
    "    guild_chat_session = initialize_guild_chat(guild_system_protocol)\n",
    "    if guild_chat_session:\n",
    "        print(\"‚úÖ  Initialization Complete. AI is calibrated to Guild Protocols.\")\n",
    "        print(\"‚û°Ô∏è  Proceed to run Cell 2 to begin the chat session.\")\n",
    "else:\n",
    "    print(\"üî¥  Initialization Failed. Please check errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647e48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .widget-text textarea { width: 100% !important; }\n",
       "    .widget-button button { background-color: #4CAF50 !important; color: white !important; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Chat Interface Initialized. Enter your prompt below.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f3635666a34ae2997ad9947289d140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a3db614ba443288e8c6098d8a6cde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', placeholder='Enter your message...'), Button(description='Send', style=ButtonSty‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cce22b15f84df5b8c2fc762bf816d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p><i>‚ö´Ô∏è [Guild AI]: Compiling response...</i></p>', layout=Layout(display='none'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2: UI Setup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create the UI components\n",
    "chat_history_view = widgets.Output(layout={'border': '1px solid black', 'height': '400px', 'overflow_y': 'auto'})\n",
    "prompt_input = widgets.Text(placeholder='Enter your message...')\n",
    "send_button = widgets.Button(description='Send')\n",
    "loading_spinner = widgets.HTML(\"<p><i>‚ö´Ô∏è [Guild AI]: Compiling response...</i></p>\", layout={'display': 'none'})\n",
    "\n",
    "def display_message(who, message, is_html=False):\n",
    "    \"\"\"Appends a message to the chat history view.\"\"\"\n",
    "    \n",
    "    if is_html:\n",
    "        # If the message is already HTML, just add the sender info and display directly.\n",
    "        # This prevents wrapping the AI's markdown-converted HTML in another <p> tag.\n",
    "        color = \"green\"\n",
    "        icon = \"üü¢\"\n",
    "        formatted_message = f'<div style=\"margin-left: 5px;\"><p style=\"color:{color}; margin-bottom: 0px;\"><strong>{icon} [Guild AI]:</strong></p>{message}</div>'\n",
    "    else:\n",
    "        # For plain text messages (like the user's), wrap them in a <p> tag as before.\n",
    "        color = \"royalblue\"\n",
    "        icon = \"üîµ\"\n",
    "        formatted_message = f'<p style=\"color:{color}; margin-left: 5px;\"><strong>{icon} [{who}]:</strong> {message}</p>'\n",
    "    \n",
    "    # The IPython.display.HTML object is used to render the string as HTML\n",
    "    chat_history_view.append_display_data(HTML(formatted_message))\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .widget-text textarea { width: 100% !important; }\n",
    "    .widget-button button { background-color: #4CAF50 !important; color: white !important; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"‚úÖ  Chat Interface Initialized. Enter your prompt below.\")\n",
    "display(chat_history_view, widgets.HBox([prompt_input, send_button]), loading_spinner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da284182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: UI Logic (The \"Backend\" for the UI)\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    prompt = prompt_input.value\n",
    "    if not prompt.strip():\n",
    "        return # Do nothing if input is empty\n",
    "    \n",
    "    # Display the user's message and show loading spinner\n",
    "    display_message(\"Operative Kin-Caid\", prompt)\n",
    "    prompt_input.value = \"\" # Clear the input box\n",
    "    loading_spinner.layout.display = 'block'\n",
    "    \n",
    "    try:\n",
    "        # Send message to the AI\n",
    "        response = guild_chat_session.send_message(prompt)\n",
    "        \n",
    "        # Display AI's response (using HTML to render markdown)\n",
    "        from markdown import markdown\n",
    "        html_response = markdown(response.text)\n",
    "        display_message(\"Guild AI\", html_response, is_html=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        display_message(\"SYSTEM\", f\"<strong>üî¥ ERROR:</strong> {e}\")\n",
    "    finally:\n",
    "        # Hide loading spinner\n",
    "        loading_spinner.layout.display = 'none'\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c482c08",
   "metadata": {},
   "source": [
    "Use this to generate chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59233f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chat_history(guild_chat_session, 'chat_logs/portfolio_project_chat.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
