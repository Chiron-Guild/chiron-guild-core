{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4072e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Current Working Directory is: C:\\Users\\kincaidn\\OneDrive - City of Austin\\Documents\\GitHub\\chiron-guild-core\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# Diagnostics\n",
    "\n",
    "import os\n",
    "print(f\"The Current Working Directory is: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è  Initializing Guild Interface AI...\n",
      "‚úÖ  Project root found at: C:\\Users\\kincaidn\\OneDrive - City of Austin\\Documents\\GitHub\\chiron-guild-core\n",
      "üî¥ ERROR: Protocol file not found at 'C:\\Users\\kincaidn\\OneDrive - City of Austin\\Documents\\GitHub\\chiron-guild-core\\_Admin & Core Docs\\Protocols-Frameworks\\prompts\\guild_oracle_prompt.md'.\n",
      "‚úÖ  Context file loaded: _Admin & Core Docs\\Protocols-Frameworks\\Copilot_Context_Protocol.md\n",
      "üî¥ ERROR loading or parsing history file: name 'file_path' is not defined\n",
      "üî¥  Initialization Failed. One or more core protocol files could not be loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Initialization (Local Environment Version)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables from the .env file in the root directory\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"üî¥ ERROR: GEMINI_API_KEY not found in .env file.\")\n",
    "    print(\"Please create a .env file in the repository root with GEMINI_API_KEY='your-key'.\")\n",
    "else:\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "# --- Guild Protocol Functions ---\n",
    "\n",
    "def find_project_root(marker_file='.gitignore'):\n",
    "    \"\"\"Finds the project's root directory by searching upwards for a marker file.\"\"\"\n",
    "    current_path = os.getcwd()\n",
    "    while current_path != os.path.dirname(current_path): # Stop at the filesystem root\n",
    "        if marker_file in os.listdir(current_path):\n",
    "            return current_path\n",
    "        current_path = os.path.dirname(current_path)\n",
    "    raise FileNotFoundError(f\"Project root marker '{marker_file}' not found.\")\n",
    "\n",
    "def load_protocol_file(root_path, relative_file_path):\n",
    "    \"\"\"Loads a specific protocol document using an absolute path from the project root.\"\"\"\n",
    "    full_path = os.path.join(root_path, relative_file_path)\n",
    "    try:\n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            print(f\"‚úÖ  Context file loaded: {relative_file_path}\")\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üî¥ ERROR: Protocol file not found at '{full_path}'.\")\n",
    "        return None\n",
    "\n",
    "def save_chat_history(chat_session, file_path):\n",
    "    \"\"\"Saves the chat history to a JSON file.\"\"\"\n",
    "    history = [\n",
    "        {'role': msg.role, 'parts': [part.text for part in msg.parts]}\n",
    "        for msg in chat_session.history\n",
    "    ]\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        print(f\"‚úÖ Chat history saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üî¥ ERROR saving history: {e}\")\n",
    "\n",
    "def load_chat_history(root_path, relative_file_path):\n",
    "    \"\"\"Loads and sanitizes a chat history from a JSON file.\"\"\"\n",
    "    full_path = os.path.join(root_path, relative_file_path)\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            history = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üü° INFO: No previous history found at {file_path}. Starting a new session.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"üî¥ ERROR loading or parsing history file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- THIS IS THE CRITICAL SANITIZATION STEP ---\n",
    "    valid_roles = ['user', 'model']\n",
    "    sanitized_history = []\n",
    "    \n",
    "    for message in history:\n",
    "        if isinstance(message, dict) and 'role' in message and message['role'] in ['user', 'model']:\n",
    "            sanitized_history.append(message)\n",
    "    return sanitized_history\n",
    "\n",
    "    # Ensure the history alternates correctly between user and model\n",
    "    if sanitized_history:\n",
    "        # The first message must be from a 'user'\n",
    "        if sanitized_history[0]['role'] != 'user':\n",
    "            print(\"üî¥ ERROR: Sanitized history does not start with a 'user' role. Starting a new session.\")\n",
    "            return None\n",
    "\n",
    "    print(f\"‚úÖ History sanitized. Loaded {len(sanitized_history)} valid messages.\")\n",
    "    return sanitized_history\n",
    "\n",
    "# The full, corrected function in Cell 1\n",
    "\n",
    "def initialize_guild_chat(system_instruction, history=None):\n",
    "    \"\"\"Initializes a new chat session with system instruction and optional history.\"\"\"\n",
    "    \n",
    "    # Define the generation_config for more detailed responses\n",
    "    generation_config = {\n",
    "      \"temperature\": 0.7,\n",
    "      \"top_p\": 1,\n",
    "      \"top_k\": 32,\n",
    "      \"max_output_tokens\": 8192,\n",
    "    }\n",
    "\n",
    "    # This is the corrected line\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.5-flash-preview-05-20\",\n",
    "        system_instruction=system_instruction,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    \n",
    "    # Pass the loaded history to the start_chat method\n",
    "    chat = model.start_chat(history=history if history else None)\n",
    "    return chat\n",
    "    \n",
    "    # Pass the loaded history to the start_chat method\n",
    "    chat = model.start_chat(history=history if history else None)\n",
    "    return chat\n",
    "    \n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32,\n",
    "        \"max_output_tokens\": 8192, # Increase the max token output significantly\n",
    "}\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.5-flash-preview-05-20\", # Corrected Model Name\n",
    "        system_instruction=system_instruction,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    chat = model.start_chat()\n",
    "    return chat\n",
    "\n",
    "# --- Main Initialization Step ---\n",
    "print(\"üõ°Ô∏è  Initializing Guild Interface AI...\")\n",
    "\n",
    "try:\n",
    "    # Find the project root directory dynamically\n",
    "    PROJECT_ROOT = find_project_root()\n",
    "    print(f\"‚úÖ  Project root found at: {PROJECT_ROOT}\")\n",
    "\n",
    "    # Define paths relative to the project root\n",
    "    ORACLE_PROMPT_PATH = os.path.join(\"_Admin & Core Docs\", \"Protocols-Frameworks\", \"guild_oracle_prompt.md\")\n",
    "    COPILOT_CONTEXT_PATH = os.path.join(\"_Admin & Core Docs\", \"Protocols-Frameworks\", \"Copilot_Context_Protocol.md\")\n",
    "    CHAT_LOG_PATH = os.path.join(\"chat_logs\", \"portfolio_project_chat.json\")\n",
    "    loaded_history = load_chat_history(PROJECT_ROOT, CHAT_LOG_PATH)\n",
    "\n",
    "    # Load all necessary components using the root path\n",
    "    oracle_persona_prompt = load_protocol_file(PROJECT_ROOT, ORACLE_PROMPT_PATH)\n",
    "    copilot_foundational_context = load_protocol_file(PROJECT_ROOT, COPILOT_CONTEXT_PATH)\n",
    "\n",
    "    # Combine the prompts into a single, comprehensive system instruction\n",
    "    if oracle_persona_prompt and copilot_foundational_context:\n",
    "        full_system_instruction = f\"\"\"\n",
    "{oracle_persona_prompt}\n",
    "\n",
    "---\n",
    "## Supplemental Foundational Context\n",
    "Below is the foundational context for the Chiron Guild, which you must use to inform all your responses.\n",
    "{copilot_foundational_context}\n",
    "\"\"\"\n",
    "        \n",
    "        # Pass the combined instruction and loaded history to the session\n",
    "        guild_chat_session = initialize_guild_chat(full_system_instruction, history=loaded_history)\n",
    "        \n",
    "        if guild_chat_session:\n",
    "            print(\"‚úÖ  Initialization Complete. Oracle is calibrated and online.\")\n",
    "            print(\"‚û°Ô∏è  Proceed to run the UI cells to begin the chat session.\")\n",
    "    else:\n",
    "        print(\"üî¥  Initialization Failed. One or more core protocol files could not be loaded.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"üî¥ CRITICAL ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: UI Setup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create the UI components\n",
    "chat_history_view = widgets.Output(layout={'border': '1px solid black', 'height': '400px', 'overflow_y': 'auto'})\n",
    "prompt_input = widgets.Text(placeholder='Enter your message...')\n",
    "send_button = widgets.Button(description='Send')\n",
    "loading_spinner = widgets.HTML(\"<p><i>‚ö´Ô∏è [Guild AI]: Compiling response...</i></p>\", layout={'display': 'none'})\n",
    "\n",
    "def display_message(who, message, is_html=False):\n",
    "    \"\"\"Appends a message to the chat history view.\"\"\"\n",
    "    \n",
    "    if is_html:\n",
    "        # If the message is already HTML, just add the sender info and display directly.\n",
    "        # This prevents wrapping the AI's markdown-converted HTML in another <p> tag.\n",
    "        color = \"green\"\n",
    "        icon = \"üü¢\"\n",
    "        formatted_message = f'<div style=\"margin-left: 5px;\"><p style=\"color:{color}; margin-bottom: 0px;\"><strong>{icon} [Guild AI]:</strong></p>{message}</div>'\n",
    "    else:\n",
    "        # For plain text messages (like the user's), wrap them in a <p> tag as before.\n",
    "        color = \"royalblue\"\n",
    "        icon = \"üîµ\"\n",
    "        formatted_message = f'<p style=\"color:{color}; margin-left: 5px;\"><strong>{icon} [{who}]:</strong> {message}</p>'\n",
    "    \n",
    "    # The IPython.display.HTML object is used to render the string as HTML\n",
    "    chat_history_view.append_display_data(HTML(formatted_message))\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .widget-text textarea { width: 100% !important; }\n",
    "    .widget-button button { background-color: #4CAF50 !important; color: white !important; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"‚úÖ  Chat Interface Initialized. Enter your prompt below.\")\n",
    "display(chat_history_view, widgets.HBox([prompt_input, send_button]), loading_spinner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2b8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Context Toolkit\n",
    "\n",
    "CONTEXT_FILES = {\n",
    "    \"manifesto\": \"_Admin & Core Docs/Protocols-Frameworks/GUILD_MANIFESTO.md\",\n",
    "    \"taxonomy\": \"_Admin & Core Docs/Protocols-Frameworks/taxonomy_framework.md\",\n",
    "    \"decomposition\": \"project_decomposition.md\",\n",
    "    \"op_protocols\": \"_Admin & Core Docs/Protocols-Frameworks/GUILD_OP_PROTOCOLS.md\",\n",
    "    \"registry\": \"_Admin & Core Docs/registry/operative_registry.json\"\n",
    "}\n",
    "\n",
    "def get_context(keyword):\n",
    "    \"\"\"Retrieves the content of a specific context file.\"\"\"\n",
    "    file_path = CONTEXT_FILES.get(keyword.lower())\n",
    "    if not file_path:\n",
    "        return f\"ERROR: Unknown context keyword '{keyword}'. Valid keywords are: {list(CONTEXT_FILES.keys())}\"\n",
    "    \n",
    "    try:\n",
    "        # Adjust pathing as needed\n",
    "        full_path = os.path.join(os.getcwd(), '..', file_path)\n",
    "        if not os.path.exists(full_path):\n",
    "             full_path = os.path.join(os.getcwd(), file_path)\n",
    "        \n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: Could not load file for '{keyword}': {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da284182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: UI Logic (The \"Backend\" for the UI)\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    prompt = prompt_input.value\n",
    "    if not prompt.strip():\n",
    "        return # Do nothing if input is empty\n",
    "    \n",
    "    # Display the user's message and show loading spinner\n",
    "    display_message(\"Operative Kin-Caid\", prompt)\n",
    "    prompt_input.value = \"\" # Clear the input box\n",
    "    loading_spinner.layout.display = 'block'\n",
    "\n",
    "    final_prompt = prompt\n",
    "    \n",
    "    if prompt.strip().lower().startswith('!context'):\n",
    "        parts = prompt.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            keyword = parts[1]\n",
    "            \n",
    "            # Extract the actual user question, which is everything AFTER the command and keyword\n",
    "            actual_question = \" \".join(parts[2:])\n",
    "            if not actual_question:\n",
    "                actual_question = f\"Please summarize the key points of the '{keyword}' document.\" # Default question\n",
    "\n",
    "            context_content = get_context(keyword)\n",
    "            display_message(\"SYSTEM\", f\"<i>Injecting context from '{keyword}'...</i>\")\n",
    "\n",
    "            # Create a new, more forceful prompt structure\n",
    "            final_prompt = f\"\"\"\n",
    "An Operative has asked the following question:\n",
    "\"{actual_question}\"\n",
    "\n",
    "To formulate your answer, you MUST use the following reference document that has been loaded into your context for this specific turn. Do not claim you cannot access it; it is provided below.\n",
    "\n",
    "<CONTEXT_DOCUMENT>\n",
    "---\n",
    "FILE: {keyword}\n",
    "---\n",
    "{context_content}\n",
    "</CONTEXT_DOCUMENT>\n",
    "\n",
    "Based on the content of the provided document, answer the Operative's question.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Send message to the AI\n",
    "        response = guild_chat_session.send_message(final_prompt)\n",
    "        \n",
    "        # Display AI's response (using HTML to render markdown)\n",
    "        from markdown import markdown\n",
    "        html_response = markdown(response.text)\n",
    "        display_message(\"Guild AI\", html_response, is_html=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        display_message(\"SYSTEM\", f\"<strong>üî¥ ERROR:</strong> {e}\")\n",
    "    finally:\n",
    "        # Hide loading spinner\n",
    "        loading_spinner.layout.display = 'none'\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bdb16",
   "metadata": {},
   "source": [
    "Use these commands at the start of a chat to add context docs:\n",
    "\n",
    "!context manifesto\n",
    "!context taxonomy\n",
    "!context decomposition\n",
    "!context op_protocols\n",
    "!context registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c482c08",
   "metadata": {},
   "source": [
    "Use this to generate chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59233f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ ERROR saving history: [Errno 2] No such file or directory: 'chat_logs/portfolio_project_chat.json'\n"
     ]
    }
   ],
   "source": [
    "save_chat_history(guild_chat_session, 'chat_logs/portfolio_project_chat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06157ed3-a7c2-4876-b386-7c254c30e32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chiron Guild Environment",
   "language": "python",
   "name": "chiron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
