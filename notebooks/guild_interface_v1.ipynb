{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b1cfb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 1: Setup & Initialization (Local Environment Version)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- Configuration ---\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load environment variables from the .env file in the root directory\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Initialization (Local Environment Version)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables from the .env file in the root directory\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"üî¥ ERROR: GEMINI_API_KEY not found in .env file.\")\n",
    "    print(\"Please create a .env file in the repository root with GEMINI_API_KEY='your-key'.\")\n",
    "else:\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "# --- Guild Protocol Functions ---\n",
    "\n",
    "def load_system_instruction(protocol_file_path=\"_Admin & Core Docs/Protocols-Frameworks/Copilot_Context_Protocol.md\"):\n",
    "    \"\"\"Loads the Guild's core protocol document to be used as the system instruction.\"\"\"\n",
    "    try:\n",
    "        # Use os.path.join for cross-platform compatibility\n",
    "        full_path = os.path.join(os.getcwd(), '..', protocol_file_path) # Assumes notebook is in notebooks/\n",
    "        if not os.path.exists(full_path):\n",
    "             full_path = os.path.join(os.getcwd(), protocol_file_path) # Assumes notebook is in root\n",
    "        \n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            print(f\"‚úÖ Protocol file found and loaded from: {full_path}\")\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üî¥ ERROR: Protocol file not found.\")\n",
    "        print(\"Please ensure the path in the 'load_system_instruction' function is correct relative to the repository root.\")\n",
    "        return None\n",
    "\n",
    "def save_chat_history(chat_session, file_path):\n",
    "    \"\"\"Saves the chat history to a JSON file.\"\"\"\n",
    "    history = [\n",
    "        {'role': msg.role, 'parts': [part.text for part in msg.parts]}\n",
    "        for msg in chat_session.history\n",
    "    ]\n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(history, f, indent=2)\n",
    "        print(f\"‚úÖ Chat history saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üî¥ ERROR saving history: {e}\")\n",
    "\n",
    "def load_chat_history(file_path):\n",
    "    \"\"\"Loads a chat history from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"üü° INFO: No previous history found at {file_path}. Starting a new session.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"üî¥ ERROR loading history: {e}\")\n",
    "        return None\n",
    "\n",
    "def initialize_guild_chat(system_instruction, history=None):\n",
    "    \"\"\"Initializes a new chat session with system instruction and optional history.\"\"\"\n",
    "    # ... (the generation_config and model setup remains the same) ...\n",
    "    model = genai.GenerativeModel( ... ) \n",
    "    \n",
    "    # Pass the loaded history to the start_chat method\n",
    "    chat = model.start_chat(history=history if history else None)\n",
    "    return chat\n",
    "    \n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32,\n",
    "        \"max_output_tokens\": 8192, # Increase the max token output significantly\n",
    "}\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-2.5-flash-preview-05-20\", # Corrected Model Name\n",
    "        system_instruction=system_instruction,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    chat = model.start_chat()\n",
    "    return chat\n",
    "\n",
    "# --- Main Initialization Step ---\n",
    "print(\"üõ°Ô∏è  Initializing Guild Interface AI...\")\n",
    "guild_system_protocol = load_system_instruction()\n",
    "\n",
    "if guild_system_protocol:\n",
    "    guild_chat_session = initialize_guild_chat(guild_system_protocol)\n",
    "    if guild_chat_session:\n",
    "        print(\"‚úÖ  Initialization Complete. AI is calibrated to Guild Protocols.\")\n",
    "        print(\"‚û°Ô∏è  Proceed to run Cell 2 to begin the chat session.\")\n",
    "else:\n",
    "    print(\"üî¥  Initialization Failed. Please check errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647e48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  Chat Session Active. Type 'exit' or 'quit' to end.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: UI Setup\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create the UI components\n",
    "chat_history_view = widgets.Output(layout={'border': '1px solid black', 'height': '400px', 'overflow_y': 'auto'})\n",
    "prompt_input = widgets.Text(placeholder='Enter your message...')\n",
    "send_button = widgets.Button(description='Send')\n",
    "loading_spinner = widgets.HTML(\"<p><i>‚ö´Ô∏è [Guild AI]: Compiling response...</i></p>\", layout={'display': 'none'})\n",
    "\n",
    "def display_message(who, message, is_html=False):\n",
    "    \"\"\"Appends a message to the chat history view.\"\"\"\n",
    "    color = \"royalblue\" if who == \"Operative Kin-Caid\" else \"green\"\n",
    "    icon = \"üîµ\" if who == \"Operative Kin-Caid\" else \"üü¢\"\n",
    "    \n",
    "    formatted_message = f'<p style=\"color:{color}; margin-left: 5px;\"><strong>{icon} [{who}]:</strong> {message}</p>'\n",
    "    chat_history_view.append_stdout(formatted_message)\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .widget-text textarea { width: 100% !important; }\n",
    "    .widget-button button { background-color: #4CAF50 !important; color: white !important; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "print(\"‚úÖ  Chat Interface Initialized. Enter your prompt below.\")\n",
    "display(chat_history_view, widgets.HBox([prompt_input, send_button]), loading_spinner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da284182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: UI Logic (The \"Backend\" for the UI)\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    prompt = prompt_input.value\n",
    "    if not prompt.strip():\n",
    "        return # Do nothing if input is empty\n",
    "    \n",
    "    # Display the user's message and show loading spinner\n",
    "    display_message(\"Operative Kin-Caid\", prompt)\n",
    "    prompt_input.value = \"\" # Clear the input box\n",
    "    loading_spinner.layout.display = 'block'\n",
    "    \n",
    "    try:\n",
    "        # Send message to the AI\n",
    "        response = guild_chat_session.send_message(prompt)\n",
    "        \n",
    "        # Display AI's response (using HTML to render markdown)\n",
    "        from markdown import markdown\n",
    "        html_response = markdown(response.text)\n",
    "        display_message(\"Guild AI\", html_response, is_html=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        display_message(\"SYSTEM\", f\"<strong>üî¥ ERROR:</strong> {e}\")\n",
    "    finally:\n",
    "        # Hide loading spinner\n",
    "        loading_spinner.layout.display = 'none'\n",
    "\n",
    "send_button.on_click(on_send_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c482c08",
   "metadata": {},
   "source": [
    "Use this to generate chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59233f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chat_history(guild_chat_session, 'chat_logs/portfolio_project_chat.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
